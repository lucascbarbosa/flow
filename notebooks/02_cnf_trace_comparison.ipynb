{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2: CNF com Trace Exato\n",
        "\n",
        "Este notebook implementa CNF completo com cálculo de log-likelihood via change of variables.\n",
        "\n",
        "## Objetivos:\n",
        "1. Implementar divergence_exact e CNF\n",
        "2. Treinar em dados 2D e MNIST reduzido\n",
        "3. Comparar com Real NVP baseline\n",
        "4. Analisar escalabilidade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from src.models.vector_field import VectorField\n",
        "from src.models.cnf import CNF\n",
        "from src.utils.datasets import Synthetic2D, MNISTReduced, get_dataloader\n",
        "from src.utils.training import train_cnf\n",
        "from src.utils.visualization import plot_data_distribution\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. CNF em Dados 2D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Treinando CNF em dados 2D...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/50:   0%|          | 0/40 [00:00<?, ?it/s]/home/lucas-barbosa/.virtualenvs/flow/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Epoch 1/50:   0%|          | 0/40 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m optimizer_2d = optim.Adam(cnf_2d.parameters(), lr=\u001b[32m1e-3\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTreinando CNF em dados 2D...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mtrain_cnf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnf_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Avaliar log-likelihood\u001b[39;00m\n\u001b[32m     14\u001b[39m cnf_2d.eval()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Projetos/flow/notebooks/../src/utils/training.py:97\u001b[39m, in \u001b[36mtrain_cnf\u001b[39m\u001b[34m(model, dataloader, optimizer, device, num_epochs)\u001b[39m\n\u001b[32m     94\u001b[39m optimizer.zero_grad()\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Calculate log-likelihood\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m log_prob = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Loss: negative log-likelihood\u001b[39;00m\n\u001b[32m    100\u001b[39m loss = -log_prob.mean()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Projetos/flow/notebooks/../src/models/cnf.py:151\u001b[39m, in \u001b[36mCNF.log_prob\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    148\u001b[39m     x = x.clone().requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# Transform x -> z\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m z, log_det = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# log p(x) = log p(z) + log |det(∂z/∂x)|\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# Since we integrate from x to z, log_det is log |det(∂z/∂x)|\u001b[39;00m\n\u001b[32m    155\u001b[39m log_prob_z = \u001b[38;5;28mself\u001b[39m.base_dist.log_prob(z)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Projetos/flow/notebooks/../src/models/cnf.py:117\u001b[39m, in \u001b[36mCNF.forward\u001b[39m\u001b[34m(self, x, reverse)\u001b[39m\n\u001b[32m    114\u001b[39m state_init = torch.cat([x, log_det_init], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# Integrate augmented ODE\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m state_t = \u001b[43modeint_adjoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_augmented_dynamics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt_span\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43madjoint_params\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Final state\u001b[39;00m\n\u001b[32m    128\u001b[39m state_final = state_t[-\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# (batch, features + 1)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torchdiffeq/_impl/adjoint.py:206\u001b[39m, in \u001b[36modeint_adjoint\u001b[39m\u001b[34m(func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, adjoint_params)\u001b[39m\n\u001b[32m    203\u001b[39m state_norm = options[\u001b[33m\"\u001b[39m\u001b[33mnorm\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    204\u001b[39m handle_adjoint_norm_(adjoint_options, shapes, state_norm)\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m ans = \u001b[43mOdeintAdjointMethod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjoint_rtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjoint_atol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m                                \u001b[49m\u001b[43madjoint_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjoint_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madjoint_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    210\u001b[39m     solution = ans\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torch/autograd/function.py:581\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    579\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    580\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    584\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    586\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    587\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    588\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    589\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torchdiffeq/_impl/adjoint.py:24\u001b[39m, in \u001b[36mOdeintAdjointMethod.forward\u001b[39m\u001b[34m(ctx, shapes, func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, t_requires_grad, *adjoint_params)\u001b[39m\n\u001b[32m     21\u001b[39m ctx.event_mode = event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     ans = \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevent_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     27\u001b[39m         y = ans\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torchdiffeq/_impl/odeint.py:80\u001b[39m, in \u001b[36modeint\u001b[39m\u001b[34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[39m\n\u001b[32m     77\u001b[39m solver = SOLVERS[method](func=func, y0=y0, rtol=rtol, atol=atol, **options)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     solution = \u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     82\u001b[39m     event_t, solution = solver.integrate_until_event(t[\u001b[32m0\u001b[39m], event_fn)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torchdiffeq/_impl/solvers.py:32\u001b[39m, in \u001b[36mAdaptiveStepsizeODESolver.integrate\u001b[39m\u001b[34m(self, t)\u001b[39m\n\u001b[32m     30\u001b[39m solution[\u001b[32m0\u001b[39m] = \u001b[38;5;28mself\u001b[39m.y0\n\u001b[32m     31\u001b[39m t = t.to(\u001b[38;5;28mself\u001b[39m.dtype)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_before_integrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(t)):\n\u001b[32m     34\u001b[39m     solution[i] = \u001b[38;5;28mself\u001b[39m._advance(t[i])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torchdiffeq/_impl/rk_common.py:213\u001b[39m, in \u001b[36mRKAdaptiveStepsizeODESolver._before_integrate\u001b[39m\u001b[34m(self, t)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_before_integrate\u001b[39m(\u001b[38;5;28mself\u001b[39m, t):\n\u001b[32m    212\u001b[39m     t0 = t[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     f0 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.first_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    215\u001b[39m         first_step = _select_initial_step(\u001b[38;5;28mself\u001b[39m.func, t[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.y0, \u001b[38;5;28mself\u001b[39m.order - \u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.rtol, \u001b[38;5;28mself\u001b[39m.atol,\n\u001b[32m    216\u001b[39m                                           \u001b[38;5;28mself\u001b[39m.norm, f0=f0)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torchdiffeq/_impl/misc.py:197\u001b[39m, in \u001b[36m_PerturbFunc.forward\u001b[39m\u001b[34m(self, t, y, perturb)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torchdiffeq/_impl/misc.py:197\u001b[39m, in \u001b[36m_PerturbFunc.forward\u001b[39m\u001b[34m(self, t, y, perturb)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Projetos/flow/notebooks/../src/models/cnf.py:72\u001b[39m, in \u001b[36mCNF._augmented_dynamics\u001b[39m\u001b[34m(self, t, state)\u001b[39m\n\u001b[32m     69\u001b[39m dx_dt = \u001b[38;5;28mself\u001b[39m.vf(t, x)  \u001b[38;5;66;03m# (batch, features)\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Compute trace of Jacobian\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m trace = \u001b[43mdivergence_exact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch,)\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# d(log_det)/dt = -trace (note the sign!)\u001b[39;00m\n\u001b[32m     75\u001b[39m dlogdet_dt = -trace.unsqueeze(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (batch, 1)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Projetos/flow/notebooks/../src/utils/trace.py:37\u001b[39m, in \u001b[36mdivergence_exact\u001b[39m\u001b[34m(f, x)\u001b[39m\n\u001b[32m     33\u001b[39m trace = torch.zeros(batch_size, device=x.device, dtype=x.dtype)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dim):\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Compute ∂f[:, i]/∂x\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     df_i = \u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mf_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (batch, dim)\u001b[39;00m\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# Sum only the diagonal: ∂f_i/∂x_i\u001b[39;00m\n\u001b[32m     45\u001b[39m     trace += df_i[:, i]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torch/autograd/__init__.py:503\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    499\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    500\u001b[39m         grad_outputs_\n\u001b[32m    501\u001b[39m     )\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    514\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    515\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    516\u001b[39m     ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "# Criar dataset 2D\n",
        "dataset_2d = Synthetic2D(n_samples=5000, noise=0.05, dataset_type='moons')\n",
        "dataloader_2d = get_dataloader(dataset_2d, batch_size=128, shuffle=True)\n",
        "\n",
        "# Criar e treinar CNF\n",
        "vf_2d = VectorField(features=2, hidden_dims=[64, 64], time_embed_dim=16)\n",
        "cnf_2d = CNF(vf_2d).to(device)\n",
        "optimizer_2d = optim.Adam(cnf_2d.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"Treinando CNF em dados 2D...\")\n",
        "train_cnf(cnf_2d, dataloader_2d, optimizer_2d, device, num_epochs=50)\n",
        "\n",
        "# Avaliar log-likelihood\n",
        "cnf_2d.eval()\n",
        "with torch.no_grad():\n",
        "    test_data = dataset_2d.data[:1000].to(device)\n",
        "    log_probs = cnf_2d.log_prob(test_data)\n",
        "    print(f\"Log-likelihood médio: {log_probs.mean().item():.4f}\")\n",
        "\n",
        "# Gerar amostras\n",
        "samples = cnf_2d.sample(1000)\n",
        "samples_np = samples.cpu().numpy()\n",
        "\n",
        "# Visualizar\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "plot_data_distribution(dataset_2d.data, ax=axes[0], title=\"Dados Originais\")\n",
        "axes[1].scatter(samples_np[:, 0], samples_np[:, 1], alpha=0.5, s=10)\n",
        "axes[1].set_title(\"Amostras Geradas\")\n",
        "axes[1].set_xlabel('x1')\n",
        "axes[1].set_ylabel('x2')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].axis('equal')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. CNF em MNIST Reduzido\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar dataset MNIST reduzido (100 dimensões)\n",
        "print(\"Carregando MNIST reduzido...\")\n",
        "dataset_mnist = MNISTReduced(train=True, n_components=100)\n",
        "dataloader_mnist = get_dataloader(dataset_mnist, batch_size=128, shuffle=True)\n",
        "\n",
        "# Criar e treinar CNF\n",
        "vf_mnist = VectorField(features=100, hidden_dims=[128, 128], time_embed_dim=32)\n",
        "cnf_mnist = CNF(vf_mnist).to(device)\n",
        "optimizer_mnist = optim.Adam(cnf_mnist.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"Treinando CNF em MNIST reduzido...\")\n",
        "start_time = time.time()\n",
        "train_cnf(cnf_mnist, dataloader_mnist, optimizer_mnist, device, num_epochs=10)\n",
        "train_time = time.time() - start_time\n",
        "print(f\"Tempo de treinamento: {train_time:.2f} segundos\")\n",
        "\n",
        "# Avaliar log-likelihood\n",
        "cnf_mnist.eval()\n",
        "with torch.no_grad():\n",
        "    test_data = dataset_mnist.data[:1000].to(device)\n",
        "    log_probs = cnf_mnist.log_prob(test_data)\n",
        "    print(f\"Log-likelihood médio: {log_probs.mean().item():.4f}\")\n",
        "\n",
        "# Tempo de sampling\n",
        "start_time = time.time()\n",
        "samples = cnf_mnist.sample(1000)\n",
        "sample_time = time.time() - start_time\n",
        "print(f\"Tempo de sampling (1000 amostras): {sample_time:.2f} segundos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Análise de Escalabilidade\n",
        "\n",
        "**Por que trace exato não escala para MNIST completo (784 dim)?**\n",
        "\n",
        "- Custo computacional: O(d²) onde d é a dimensão\n",
        "- Para d=784: precisamos de 784 backward passes\n",
        "- Cada backward é O(784) → Total O(784²) = O(614,656)\n",
        "- Isso é muito lento para treinamento prático!\n",
        "\n",
        "**Solução:** Usar Hutchinson trace estimator (FFJORD) - O(d)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "flow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
