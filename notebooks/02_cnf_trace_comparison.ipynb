{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2: CNF com Trace Exato\n",
        "\n",
        "Este notebook implementa CNF completo com cálculo de log-likelihood via change of variables.\n",
        "\n",
        "## Objetivos:\n",
        "1. Implementar divergence_exact e CNF\n",
        "2. Treinar em dados 2D e MNIST reduzido\n",
        "3. Comparar com Real NVP baseline\n",
        "4. Analisar escalabilidade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('..')\n",
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from src.models.cnf import CNF\n",
        "from src.models.vector_field import VectorField2D\n",
        "from src.utils.datasets import Synthetic2D, MNISTReduced, get_dataloader\n",
        "from src.utils.training import train_cnf, train_realnvp\n",
        "from src.utils.visualization import (\n",
        "    Synthetic2DViz,\n",
        "    MNISTViz,\n",
        ")\n",
        "from zuko.flows import RealNVP\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Directories for saving figures and checkpoints\n",
        "FIGURES_DIR = '../results/figures'\n",
        "CHECKPOINTS_DIR = '../results/checkpoints'\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar dataset 2D\n",
        "dataset_2d = Synthetic2D(n_samples=5000, noise=0.05, dataset_type='moons')\n",
        "dataloader_2d = get_dataloader(dataset_2d, batch_size=128, shuffle=True)\n",
        "\n",
        "# Dataset and model names for file naming\n",
        "DATASET_2D_NAME = 'moons'\n",
        "\n",
        "# Visualizar\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "Synthetic2DViz.plot_data_distribution(\n",
        "    dataset_2d.data,\n",
        "    ax=ax,\n",
        "    title=\"Dataset: Two Moons\",\n",
        "    save_path=os.path.join(FIGURES_DIR, f'02_{DATASET_2D_NAME}_data.png')\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 CNF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar e treinar CNF\n",
        "MODEL_2D_CNF_NAME = 'cnf'\n",
        "vf_2d = VectorField2D(features=2, hidden_dims=[64, 64], time_embed_dim=16)\n",
        "cnf_2d = CNF(vf_2d, trace_scale=0.75).to(device)\n",
        "optimizer_2d = optim.Adam(cnf_2d.parameters(), lr=1e-2)\n",
        "\n",
        "print(\"Treinando CNF em dados 2D...\")\n",
        "train_cnf(cnf_2d, dataloader_2d, optimizer_2d, n_epochs=100)\n",
        "\n",
        "# Salvar checkpoint\n",
        "checkpoint_path = os.path.join(\n",
        "    CHECKPOINTS_DIR,\n",
        "    f'02__{DATASET_2D_NAME}_{MODEL_2D_CNF_NAME}.pt'\n",
        ")\n",
        "torch.save(cnf_2d.state_dict(), checkpoint_path)\n",
        "print(f\"Checkpoint saved to: {checkpoint_path}\")\n",
        "\n",
        "# Avaliar log-likelihood\n",
        "cnf_2d.eval()\n",
        "with torch.no_grad():\n",
        "    test_data = dataset_2d.data[:1000].to(device)\n",
        "    log_probs = cnf_2d.log_prob(test_data)\n",
        "    print(f\"Log-likelihood médio: {log_probs.mean().item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Synthetic2DViz.plot_transformation(\n",
        "    cnf_2d,\n",
        "    n_steps=100,\n",
        "    n_samples=2000,\n",
        "    save_path=os.path.join(\n",
        "        FIGURES_DIR,\n",
        "        f'02_{DATASET_2D_NAME}_{MODEL_2D_CNF_NAME}_transformation.png'\n",
        "    )\n",
        ")\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "for i, t in enumerate([0.0, 0.5, 1.0]):\n",
        "    Synthetic2DViz.plot_vector_field(\n",
        "        cnf_2d,\n",
        "        xlim=(-5, 5),\n",
        "        ylim=(-5, 5),\n",
        "        n_grid=50,\n",
        "        t=t,\n",
        "        ax=axes[i],\n",
        "        save_path=os.path.join(\n",
        "            FIGURES_DIR,\n",
        "            f'02_{DATASET_2D_NAME}_{MODEL_2D_CNF_NAME}_vector_fields.png'\n",
        "        )\n",
        "    )\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. RealNVP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar e treinar RealNVP em dados 2D\n",
        "MODEL_2D_RNVP_NAME = 'realnvp'\n",
        "realnvp_2d = RealNVP(\n",
        "    features=2,\n",
        "    transforms=4,\n",
        "    hidden_features=[64, 64]\n",
        ").to(device)\n",
        "optimizer_rnvp_2d = optim.Adam(realnvp_2d.parameters(), lr=5e-3)\n",
        "\n",
        "print(\"Treinando RealNVP em dados 2D...\")\n",
        "train_realnvp(\n",
        "    realnvp_2d,\n",
        "    dataloader_2d,\n",
        "    optimizer_rnvp_2d,\n",
        "    device,\n",
        "    n_epochs=50\n",
        ")\n",
        "\n",
        "# Salvar checkpoint\n",
        "checkpoint_path = os.path.join(\n",
        "    CHECKPOINTS_DIR,\n",
        "    f'02__{DATASET_2D_NAME}_{MODEL_2D_RNVP_NAME}.pt'\n",
        ")\n",
        "torch.save(realnvp_2d.state_dict(), checkpoint_path)\n",
        "print(f\"Checkpoint saved to: {checkpoint_path}\")\n",
        "\n",
        "# Avaliar log-likelihood\n",
        "realnvp_2d.eval()\n",
        "with torch.no_grad():\n",
        "    test_data = dataset_2d.data[:1000].to(device).to(torch.float32)\n",
        "    dist = realnvp_2d(None)\n",
        "    log_probs = dist.log_prob(test_data)\n",
        "    print(f\"Log-likelihood médio: {log_probs.mean().item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Synthetic2DViz.plot_transformation(\n",
        "    realnvp_2d,\n",
        "    n_samples=1000,\n",
        "    save_path=os.path.join(\n",
        "        FIGURES_DIR,\n",
        "        f'02_{DATASET_2D_NAME}_{MODEL_2D_RNVP_NAME}_transformation.png'\n",
        "    )\n",
        ")\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "directions = ['forward', 'inverse', 'forward']\n",
        "for i, t in enumerate([0.0, 0.5, 1.0]):\n",
        "    Synthetic2DViz.plot_vector_field(\n",
        "        realnvp_2d,\n",
        "        xlim=(-5, 5),\n",
        "        ylim=(-5, 5),\n",
        "        t=t,\n",
        "        n_grid=50,\n",
        "        ax=axes[i],\n",
        "        save_path=os.path.join(\n",
        "            FIGURES_DIR,\n",
        "            f'02_{DATASET_2D_NAME}_{MODEL_2D_RNVP_NAME}_vector_fields.png'\n",
        "        )\n",
        "    )\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. MNIST Reduzido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Carregando MNIST reduzido...\n"
          ]
        }
      ],
      "source": [
        "# Criar dataset MNIST reduzido (100 dimensões)\n",
        "print(\"Carregando MNIST reduzido...\")\n",
        "dataset_mnist = MNISTReduced(train=True, n_components=100)\n",
        "dataloader_mnist = get_dataloader(dataset_mnist, batch_size=128, shuffle=True)\n",
        "\n",
        "# Dataset and model names for file naming\n",
        "DATASET_MNIST_NAME = 'mnist_reduced'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. CNF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Treinando CNF em MNIST reduzido...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10:   2%|▏         | 8/469 [00:43<41:56,  5.46s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTreinando CNF em MNIST reduzido...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mtrain_cnf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnf_mnist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_mnist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_mnist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m train_time = time.time() - start_time\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTempo de treinamento: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m segundos\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Projetos/flow/notebooks/../src/utils/training.py:132\u001b[39m, in \u001b[36mtrain_cnf\u001b[39m\u001b[34m(model, dataloader, optimizer, n_epochs)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# Loss: negative log-likelihood\u001b[39;00m\n\u001b[32m    130\u001b[39m loss = -log_prob.mean()\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# Gradient clipping (optional, but recommended)\u001b[39;00m\n\u001b[32m    135\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/flow/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Criar e treinar CNF\n",
        "vf_mnist = VectorField2D(\n",
        "    features=100,\n",
        "    hidden_dims=[64, 64],\n",
        "    time_embed_dim=16\n",
        ")\n",
        "cnf_mnist = CNF(vf_mnist).to(device)\n",
        "optimizer_mnist = optim.Adam(cnf_mnist.parameters(), lr=1e-4)\n",
        "\n",
        "MODEL_MNIST_CNF_NAME = 'cnf'\n",
        "print(\"Treinando CNF em MNIST reduzido...\")\n",
        "start_time = time.time()\n",
        "train_cnf(cnf_mnist, dataloader_mnist, optimizer_mnist, n_epochs=10)\n",
        "train_time = time.time() - start_time\n",
        "print(f\"Tempo de treinamento: {train_time:.2f} segundos\")\n",
        "\n",
        "# Salvar checkpoint\n",
        "checkpoint_path = os.path.join(\n",
        "    CHECKPOINTS_DIR,\n",
        "    f'02__{DATASET_MNIST_NAME}_{MODEL_MNIST_CNF_NAME}.pt'\n",
        ")\n",
        "torch.save(cnf_mnist.state_dict(), checkpoint_path)\n",
        "print(f\"Checkpoint saved to: {checkpoint_path}\")\n",
        "\n",
        "# Avaliar log-likelihood\n",
        "cnf_mnist.eval()\n",
        "with torch.no_grad():\n",
        "    test_data = dataset_mnist.data[:1000].to(device)\n",
        "    log_probs = cnf_mnist.log_prob(test_data)\n",
        "    print(f\"Log-likelihood médio: {log_probs.mean().item():.4f}\")\n",
        "\n",
        "# Tempo de sampling\n",
        "start_time = time.time()\n",
        "samples = cnf_mnist.sample(1000)\n",
        "sample_time = time.time() - start_time\n",
        "print(f\"Tempo de sampling (1000 amostras): {sample_time:.2f} segundos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit PCA on dataset first for consistent projection\n",
        "sampled_data = cnf_mnist.sample(1000)\n",
        "MNISTViz.project_to_2d(sampled_data)\n",
        "MNISTViz.plot_transformation(\n",
        "    cnf_mnist,\n",
        "    n_samples=100,\n",
        "    data_sample=sampled_data\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. RealNVP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar e treinar RealNVP em MNIST reduzido\n",
        "# RealNVP from zuko: 100 sample features, no context features\n",
        "realnvp_mnist = RealNVP(\n",
        "    features=100,\n",
        "    transforms=4,\n",
        "    hidden_features=[128, 128]\n",
        ").to(device)\n",
        "\n",
        "optimizer_rnvp_mnist = optim.Adam(realnvp_mnist.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"Treinando RealNVP em MNIST reduzido...\")\n",
        "start_time = time.time()\n",
        "train_realnvp(\n",
        "    realnvp_mnist,\n",
        "    dataloader_mnist,\n",
        "    optimizer_rnvp_mnist,\n",
        "    device,\n",
        "    n_epochs=5\n",
        ")\n",
        "train_time = time.time() - start_time\n",
        "print(f\"Tempo de treinamento: {train_time:.2f} segundos\")\n",
        "\n",
        "# Avaliar log-likelihood\n",
        "realnvp_mnist.eval()\n",
        "with torch.no_grad():\n",
        "    test_data = dataset_mnist.data[:1000].to(device)\n",
        "    dist = realnvp_mnist(None)\n",
        "    log_probs = dist.log_prob(test_data)\n",
        "    print(f\"Log-likelihood médio: {log_probs.mean().item():.4f}\")\n",
        "\n",
        "# Tempo de sampling\n",
        "start_time = time.time()\n",
        "samples = realnvp_mnist(None).sample((1000,))\n",
        "sample_time = time.time() - start_time\n",
        "print(f\"Tempo de sampling (1000 amostras): {sample_time:.2f} segundos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the same PCA already fitted\n",
        "MNISTViz.plot_transformation(\n",
        "    realnvp_mnist,\n",
        "    n_samples=100,\n",
        "    data_sample=dataset_mnist.data[:1000]\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Análise de Escalabilidade\n",
        "\n",
        "**Por que trace exato não escala para MNIST completo (784 dim)?**\n",
        "\n",
        "- Custo computacional: O(d²) onde d é a dimensão\n",
        "- Para d=784: precisamos de 784 backward passes\n",
        "- Cada backward é O(784) → Total O(784²) = O(614,656)\n",
        "- Isso é muito lento para treinamento prático!\n",
        "\n",
        "**Solução:** Usar Hutchinson trace estimator (FFJORD) - O(d)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "flow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
