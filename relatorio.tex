\documentclass[12pt,a4paper]{article}

% Pacotes essenciais
\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}

% Configurações de página
\geometry{margin=2.5cm}

% Configurações de código
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green!60!black},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    frame=single,
    breaklines=true,
    showstringspaces=false
}

% Configurações de hyperref
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Mini-Projeto 1: Neural ODEs e Continuous Normalizing Flows},
    pdfauthor={Seu Nome}
}

% Título
\title{Mini-Projeto 1: Neural ODEs e Continuous Normalizing Flows}
\author{Seu Nome \\ \small Curso/Instituição}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Este relatório apresenta a implementação e análise de Neural Ordinary Differential Equations (Neural ODEs) e Continuous Normalizing Flows (CNFs) usando a biblioteca \texttt{torchdiffeq}. Implementamos três componentes principais: (1) Neural ODE básico para aprendizado de transformações em dados 2D, (2) CNF com cálculo exato de trace para mudança de variáveis, e (3) FFJORD com estimador Hutchinson para escalabilidade em alta dimensão. Realizamos experimentos em datasets sintéticos 2D e MNIST, comparando diferentes solvers ODE, analisando trade-offs computacionais e avaliando a qualidade dos modelos gerados. Os resultados demonstram a expressividade dos CNFs, mas também evidenciam os desafios computacionais associados, especialmente o custo $\mathcal{O}(d^2)$ do trace exato versus $\mathcal{O}(d)$ do estimador Hutchinson.
\end{abstract}

\tableofcontents
\newpage

\section{Introdução}

\subsection{Contexto e Motivação}

Neural Ordinary Differential Equations (Neural ODEs) introduzidos por Chen et al. \cite{chen2018neural} representam uma abordagem inovadora para modelar transformações contínuas usando equações diferenciais ordinárias parametrizadas por redes neurais. Esta técnica permite modelar dinâmicas temporais de forma contínua, superando limitações de arquiteturas discretas tradicionais.

Continuous Normalizing Flows (CNFs) estendem essa ideia para modelagem generativa, permitindo transformar uma distribuição base simples (como uma Gaussiana) em uma distribuição complexa de dados através de um fluxo contínuo. O trabalho seminal de Chen et al. \cite{chen2018ffjord} introduziu FFJORD, uma versão escalável de CNF que utiliza estimação estocástica de trace para reduzir a complexidade computacional de $\mathcal{O}(d^2)$ para $\mathcal{O}(d)$.

\subsection{Objetivos}

Este projeto tem como objetivos:

\begin{enumerate}
    \item Implementar Neural ODEs usando \texttt{torchdiffeq} para aprendizado de transformações em dados 2D
    \item Compreender e implementar Continuous Normalizing Flows com cálculo exato de trace
    \item Aplicar técnicas de estimação de trace (Hutchinson) para escalabilidade
    \item Analisar trade-offs computacionais entre CNFs e discrete flows
    \item Preparar-se conceitualmente para Flow Matching (Módulo 2)
\end{enumerate}

\subsection{Estrutura do Relatório}

Este relatório está organizado da seguinte forma: Seção 2 apresenta os fundamentos teóricos; Seção 3 descreve a implementação; Seção 4 apresenta os experimentos e resultados; Seção 5 discute os achados; e Seção 6 conclui o trabalho.

\section{Fundamentos Teóricos}

\subsection{Neural Ordinary Differential Equations}

Neural ODEs \cite{chen2018neural} modelam transformações contínuas através de equações diferenciais ordinárias:

\begin{equation}
\frac{dx(t)}{dt} = f_\theta(x(t), t)
\end{equation}

onde $f_\theta$ é uma rede neural parametrizada por $\theta$ que define o campo vetorial. A transformação de $x(0)$ para $x(1)$ é obtida através de integração numérica:

\begin{equation}
x(1) = x(0) + \int_0^1 f_\theta(x(t), t) dt
\end{equation}

A integração reversa é feita usando o método adjoint, permitindo backpropagation eficiente através do solver ODE.

\subsection{Continuous Normalizing Flows}

CNFs \cite{chen2018ffjord} estendem Neural ODEs para modelagem generativa usando a fórmula de mudança de variáveis. Para uma transformação invertível $z = \varphi(x)$:

\begin{equation}
\log p(x) = \log p(z) + \log \left|\det \frac{\partial \varphi}{\partial x}\right|
\end{equation}

Para CNF com $\varphi_t(x)$ integrando $\frac{dx}{dt} = f(x, t)$:

\begin{equation}
\log p(x) = \log p(z) + \int_0^1 \text{tr}\left(\frac{\partial f}{\partial x}\right) dt
\end{equation}

onde $z = \varphi_0(x)$ é obtido através de integração reversa.

\subsection{FFJORD: Free-form Continuous Dynamics}

FFJORD (Free-form Continuous Dynamics) \cite{chen2018ffjord} é uma extensão escalável de CNFs que resolve o problema de complexidade computacional do cálculo exato de trace. Enquanto CNFs com trace exato têm complexidade $\mathcal{O}(d^2)$ devido à necessidade de calcular cada elemento diagonal da matriz Jacobiana, FFJORD utiliza estimação estocástica de trace para reduzir essa complexidade para $\mathcal{O}(d)$.

A principal inovação do FFJORD é a utilização do estimador Hutchinson para aproximar o trace da matriz Jacobiana. Isso permite que o modelo seja treinado eficientemente em dados de alta dimensionalidade, como imagens (por exemplo, MNIST com 784 dimensões), onde o cálculo exato seria computacionalmente proibitivo.

Além da estimação de trace, FFJORD mantém todas as vantagens dos CNFs:
\begin{itemize}
    \item Transformações contínuas e invertíveis
    \item Alta expressividade através de campos vetoriais livres (free-form)
    \item Integração numérica adaptativa usando solvers ODE
    \item Backpropagation eficiente através do método adjoint
\end{itemize}

A arquitetura do FFJORD é idêntica à de um CNF, diferindo apenas na forma como o trace é computado durante o treinamento. Isso torna FFJORD uma solução prática para aplicação de CNFs em problemas reais de alta dimensionalidade.

\subsection{Cálculo de Trace}

\subsubsection{Trace Exato}

O cálculo exato do trace requer computar cada elemento diagonal da matriz Jacobiana:

\begin{equation}
\text{tr}\left(\frac{\partial f}{\partial x}\right) = \sum_{i=1}^d \frac{\partial f_i}{\partial x_i}
\end{equation}

Isso requer $d$ passes de backpropagation, resultando em complexidade $\mathcal{O}(d^2)$.

\subsubsection{Estimador Hutchinson}

O estimador Hutchinson utiliza a identidade:

\begin{equation}
\text{tr}(A) = \mathbb{E}_{\epsilon \sim p(\epsilon)}[\epsilon^T A \epsilon]
\end{equation}

onde $\epsilon$ é um vetor aleatório. Para $A = \frac{\partial f}{\partial x}$:

\begin{equation}
\text{tr}\left(\frac{\partial f}{\partial x}\right) \approx \frac{1}{n}\sum_{i=1}^n \epsilon_i^T \frac{\partial f}{\partial x} \epsilon_i = \frac{1}{n}\sum_{i=1}^n \epsilon_i^T \frac{\partial (f^T \epsilon_i)}{\partial x}
\end{equation}

Isso pode ser computado com um único backward pass usando vector-Jacobian product (VJP), reduzindo a complexidade para $\mathcal{O}(d)$.

\section{Implementação}

\subsection{Estrutura do Projeto}

O projeto segue a seguinte estrutura:

\begin{lstlisting}[language=bash]
mini-project-1/
├── src/
│   ├── models/
│   │   ├── vector_field.py      # Arquiteturas VectorField2D
│   │   ├── neural_ode.py        # Neural ODE básico
│   │   ├── cnf.py               # CNF com trace exato
│   │   └── ffjord.py            # FFJORD com Hutchinson
│   ├── utils/
│   │   ├── datasets.py          # Data loading
│   │   ├── training.py          # Training loops
│   │   ├── trace.py             # Trace computation
│   │   └── visualization.py    # Plotting
│   └── experiments/
│       ├── exp1_ode_solvers.py
│       ├── exp2_regularization.py
│       └── exp3_architectures.py
├── notebooks/
│   ├── 01_neural_ode_2d.ipynb
│   ├── 02_cnf_trace_comparison.ipynb
│   └── 03_ffjord_mnist.ipynb
└── results/
    ├── figures/
    └── checkpoints/
\end{lstlisting}

\subsection{Vector Field}

O \texttt{VectorField2D} parametriza $\frac{dx}{dt} = f(x, t)$ usando uma rede neural. Implementamos:

\begin{itemize}
    \item \textbf{Time Embedding}: Codificação senoidal do tempo
    \item \textbf{MLP}: Rede feedforward simples ou com skip connections
    \item \textbf{Inicialização}: Última camada com pesos pequenos ($\sigma = 0.01$)
\end{itemize}

\subsubsection{Time Embedding}

Uma das características fundamentais dos Neural ODEs é a dependência temporal explícita do campo vetorial $f(x, t)$. Para permitir que a rede neural processe adequadamente o tempo $t$, utilizamos uma codificação senoidal posicional (positional encoding), inspirada nos Transformers \cite{vaswani2017attention}.

O time embedding transforma o escalar temporal $t \in [0, 1]$ em um vetor de dimensão $d_{emb}$ através de funções senoidais com frequências diferentes. Para uma dimensão de embedding $d_{emb}$, calculamos:

\begin{equation}
\text{PE}_{2i}(t) = \sin\left(\frac{t}{10000^{2i/d_{emb}}}\right)
\end{equation}

\begin{equation}
\text{PE}_{2i+1}(t) = \cos\left(\frac{t}{10000^{2i/d_{emb}}}\right)
\end{equation}

onde $i \in \{0, 1, \ldots, d_{emb}/2 - 1\}$ e $10000$ é uma constante de escala. O vetor de embedding final é a concatenação:

\begin{equation}
t_{emb} = [\text{PE}_0(t), \text{PE}_1(t), \ldots, \text{PE}_{d_{emb}-1}(t)]
\end{equation}

Esta codificação possui várias vantagens:
\begin{itemize}
    \item \textbf{Periodicidade}: As funções senoidais capturam padrões periódicos na dinâmica temporal
    \item \textbf{Suavidade}: A representação é suave e diferenciável, essencial para integração ODE
    \item \textbf{Generalização}: Permite que o modelo generalize para valores de tempo não vistos durante o treinamento
    \item \textbf{Multi-escala}: Diferentes frequências capturam diferentes escalas temporais
\end{itemize}

No forward pass do \texttt{VectorField2D}, o tempo $t$ é primeiro convertido em seu embedding $t_{emb}$, que é então concatenado com o estado $x$ antes de ser passado pela rede neural:

\begin{equation}
f(x, t) = \text{MLP}([x, t_{emb}])
\end{equation}

Utilizamos $d_{emb} = 16$ para dados 2D e $d_{emb} = 32$ para MNIST, balanceando expressividade e eficiência computacional.

\subsection{Neural ODE}

O \texttt{NeuralODE} integra o campo vetorial de $t=0$ até $t=1$ usando \texttt{odeint} do \texttt{torchdiffeq}, suportando múltiplos solvers (euler, rk4, dopri5).

\subsection{CNF com Trace Exato}

Implementamos \texttt{divergence\_exact} que calcula o trace exato iterando sobre cada dimensão:

\begin{lstlisting}[language=Python]
def divergence_exact(f, x):
    batch_size, dim = x.shape
    trace = torch.zeros(batch_size, device=device)
    for i in range(dim):
        df_i = torch.autograd.grad(
            f[:, i].sum(), x, 
            create_graph=True, 
            retain_graph=True
        )[0]
        trace += df_i[:, i]
    return trace
\end{lstlisting}

O CNF integra dinâmicas aumentadas $[x, \log\det]$ onde $\frac{d\log\det}{dt} = -\text{tr}\left(\frac{\partial f}{\partial x}\right)$.

\subsection{FFJORD com Hutchinson}

FFJORD utiliza o estimador Hutchinson para reduzir a complexidade:

\begin{lstlisting}[language=Python]
def hutchinson_trace_estimator(f, x, n_samples=1, noise='rademacher'):
    trace = torch.zeros(x.shape[0], device=device)
    for _ in range(n_samples):
        if noise == 'rademacher':
            epsilon = torch.randint(0, 2, x.shape, device=device) * 2 - 1
        else:  # gaussian
            epsilon = torch.randn_like(x)
        
        vjp = torch.autograd.grad(
            f(x).sum(), x, 
            grad_outputs=epsilon,
            create_graph=True,
            retain_graph=True
        )[0]
        trace += (epsilon * vjp).sum(-1)
    return trace / n_samples
\end{lstlisting}

\section{Datasets}

Este projeto utiliza três tipos de datasets para avaliar os modelos em diferentes níveis de complexidade e dimensionalidade. Cada dataset foi escolhido para demonstrar aspectos específicos dos Neural ODEs e CNFs.

\subsection{Dataset 2D Sintético}

Utilizamos datasets sintéticos bidimensionais para visualização e análise qualitativa dos modelos. O dataset principal é o \texttt{Synthetic2D}, que suporta três tipos de distribuições:

\begin{itemize}
    \item \textbf{Two Moons}: Distribuição em formato de duas luas entrelaçadas, gerada usando \texttt{sklearn.datasets.make\_moons}
    \item \textbf{Two Circles}: Distribuição de dois círculos concêntricos, gerada usando \texttt{sklearn.datasets.make\_circles}
    \item \textbf{Spirals}: Duas espirais entrelaçadas, geradas através de função customizada
\end{itemize}

\textbf{Configuração padrão}:
\begin{itemize}
    \item Número de amostras: 5000
    \item Ruído: 0.05
    \item Dimensão: 2
    \item Tipo de dados: \texttt{torch.float64}
\end{itemize}

Este dataset é ideal para:
\begin{itemize}
    \item Visualização direta das trajetórias e campos vetoriais
    \item Análise qualitativa da capacidade de transformação dos modelos
    \item Comparação rápida entre diferentes arquiteturas e solvers
    \item Debugging e desenvolvimento inicial dos modelos
\end{itemize}

\subsection{MNIST Reduzido}

O \texttt{MNISTReduced} aplica Análise de Componentes Principais (PCA) ao dataset MNIST original para reduzir a dimensionalidade de 784 para 100 componentes principais. Esta redução permite:

\begin{itemize}
    \item Testar CNFs com trace exato em dimensões intermediárias (100D)
    \item Comparar a escalabilidade entre trace exato e estimador Hutchinson
    \item Reduzir o tempo de treinamento para experimentos exploratórios
    \item Manter características essenciais dos dígitos através das componentes principais
\end{itemize}

\textbf{Configuração}:
\begin{itemize}
    \item Dimensão original: 784 (28×28 pixels)
    \item Dimensão reduzida: 100 componentes PCA
    \item Dataset: MNIST completo (60.000 treino, 10.000 teste)
    \item Preprocessamento: Apenas flatten e normalização para [0, 1]
    \item Tipo de dados: \texttt{torch.float64}
\end{itemize}

O PCA é aplicado no conjunto de treino e os mesmos componentes são usados para transformar o conjunto de teste, garantindo consistência. Esta abordagem preserva aproximadamente 95\% da variância dos dados originais, mantendo a informação visual relevante enquanto reduz significativamente a complexidade computacional.

\subsection{MNIST Completo}

O \texttt{MNISTComplete} utiliza o dataset MNIST em sua dimensionalidade completa (784 dimensões) com preprocessing adequado para normalizing flows. Este é o dataset utilizado para treinar o FFJORD com estimador Hutchinson.

\textbf{Preprocessamento}:

O preprocessing é crucial para normalizing flows, pois os dados devem estar em um espaço contínuo e não-degenerado. Implementamos um pipeline de três etapas:

\begin{enumerate}
    \item \textbf{Dequantização}: Transforma valores discretos $\{0, 1, \ldots, 255\}$ em contínuos $(0, 256)$ através da adição de ruído uniforme:
    \begin{equation}
    x_{dequant} = x + \mathcal{U}(0, 1), \quad x \in [0, 255]
    \end{equation}
    seguido de normalização para $(0, 1)$.
    
    \item \textbf{Logit Transform}: Aplica transformação logística para mapear $(0, 1)$ para $(-\infty, +\infty)$, evitando problemas de boundary:
    \begin{equation}
    x_{logit} = \text{logit}(\alpha + (1 - 2\alpha) \cdot x_{dequant})
    \end{equation}
    onde $\alpha = 0.05$ é um parâmetro de margem que evita valores extremos no logit.
    
    \item \textbf{Normalização}: Os dados são mantidos em formato de tensor flatten (784 dimensões).
\end{enumerate}

\textbf{Configuração}:
\begin{itemize}
    \item Dimensão: 784 (28×28 pixels)
    \item Dataset: MNIST completo (60.000 treino, 10.000 teste)
    \item Batch size: 128-256
    \item Tipo de dados: \texttt{torch.float64}
    \item Preprocessing: Dequantização + Logit transform
\end{itemize}

Este dataset é utilizado exclusivamente com FFJORD devido à necessidade do estimador Hutchinson para escalabilidade. O cálculo exato de trace seria computacionalmente proibitivo em 784 dimensões, demonstrando claramente a importância da estimação estocástica.

\subsection{Resumo Comparativo}

\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
        \toprule
        Dataset & Dimensão & Uso Principal & Trace \\
        \midrule
        2D Sintético & 2 & Visualização, Debug & Exato \\
        MNIST Reduzido & 100 & Comparação escalabilidade & Exato/Hutchinson \\
        MNIST Completo & 784 & Produção, FFJORD & Hutchinson \\
        \bottomrule
    \end{tabular}
    \caption{Comparação dos datasets utilizados}
\end{table}

\section{Experimentos e Resultados}

\subsection{Milestone 1: Neural ODE Básico}

O objetivo deste milestone é implementar e analisar um Neural ODE básico para aprendizado de transformações contínuas em dados bidimensionais. Este milestone serve como base para compreender os conceitos fundamentais antes de avançar para CNFs e FFJORD.

\subsubsection{Dataset e Configuração}

Utilizamos o dataset 2D sintético do tipo "moons" conforme descrito na Seção 4, com 5000 amostras e ruído 0.05. Este dataset é ideal para visualização direta das transformações aprendidas pelo modelo.

\subsubsection{Hiperparâmetros e Arquitetura}

A configuração completa do modelo e treinamento é apresentada na Tabela~\ref{tab:milestone1_hyperparams}.

\begin{table}[H]
    \centering
    \begin{tabular}{lc}
        \toprule
        Hiperparâmetro & Valor \\
        \midrule
        \textbf{Arquitetura} & \\
        Features (dimensão de entrada) & 2 \\
        Hidden dimensions & [64, 64] \\
        Time embedding dimension & 16 \\
        Ativação & Tanh \\
        \midrule
        \textbf{Treinamento} & \\
        Learning rate & $10^{-4}$ \\
        Optimizer & Adam \\
        Batch size & 128 \\
        Número de épocas & 200 \\
        Número de steps (integração) & 20 \\
        Número de amostras (MMD loss) & 10 \\
        \midrule
        \textbf{Solver ODE} & \\
        Método inicial & euler \\
        rtol & $10^{-5}$ \\
        atol & $10^{-5}$ \\
        \bottomrule
    \end{tabular}
    \caption{Hiperparâmetros utilizados no Milestone 1}
    \label{tab:milestone1_hyperparams}
\end{table}

\textbf{Arquitetura do Vector Field}: O \texttt{VectorField2D} utiliza uma MLP com duas camadas ocultas de 64 neurônios cada, seguida de uma camada de saída de 2 dimensões (correspondente à dimensão dos dados). O tempo é codificado através de time embedding senoidal de dimensão 16, que é concatenado com o estado antes de passar pela rede. A última camada é inicializada com pesos pequenos ($\sigma = 0.01$) para garantir que as transformações iniciais sejam suaves.

\textbf{Função de Perda}: Utilizamos Maximum Mean Discrepancy (MMD) loss com três componentes:
\begin{enumerate}
    \item \textbf{Forward loss}: Garante que dados $\rightarrow$ distribuição Gaussiana
    \item \textbf{Backward loss}: Garante que distribuição Gaussiana $\rightarrow$ dados
    \item \textbf{Reversibility loss}: Garante que forward seguido de backward retorna ao estado original (MSE)
\end{enumerate}

Esta abordagem permite que o modelo aprenda transformações bidirecionais e invertíveis, essenciais para normalizing flows.

\subsubsection{Resultados e Visualizações}

Após 200 épocas de treinamento, o modelo aprendeu transformações suaves que mapeiam os dados do dataset "moons" para uma distribuição Gaussiana. As visualizações principais são apresentadas nas Figuras~\ref{fig:milestone1_trajectories} e~\ref{fig:milestone1_vectorfield}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{results/figures/01_moons_neuralode.png}
    \caption{Trajetórias aprendidas pelo Neural ODE. À esquerda: distribuição original dos dados (transparente) com pontos iniciais $x(0)$ amostrados. À direita: estado final $x(1)$ após integração de $t=0$ até $t=1$, mostrando a transformação para distribuição mais próxima de uma Gaussiana.}
    \label{fig:milestone1_trajectories}
\end{figure}

A Figura~\ref{fig:milestone1_trajectories} demonstra que o modelo aprendeu trajetórias suaves que transformam os dados do formato "moons" para uma distribuição mais concentrada. As trajetórias são contínuas e não se cruzam, indicando que o campo vetorial aprendido é bem-comportado.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{results/figures/vector_field_2d.png}
    \caption{Campo vetorial $f(x, t)$ aprendido pelo modelo em um grid 2D para $t=0.5$. As setas indicam a direção e magnitude do campo vetorial em cada ponto do espaço. Cores mais quentes (amarelo) indicam maior magnitude.}
    \label{fig:milestone1_vectorfield}
\end{figure}

A Figura~\ref{fig:milestone1_vectorfield} mostra o campo vetorial aprendido, que define a dinâmica $\frac{dx}{dt} = f(x, t)$. O campo vetorial é suave e contínuo, com direções que guiam os pontos de dados em direção à distribuição alvo. A variação temporal do campo (não mostrada aqui) permite que o modelo adapte a dinâmica ao longo do tempo.

\subsubsection{Análise de Solvers ODE}

Uma das vantagens dos Neural ODEs é a flexibilidade na escolha do solver numérico. Comparamos três métodos diferentes para entender os trade-offs entre precisão, velocidade e número de avaliações de função (NFEs):

\begin{table}[H]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Solver & NFEs & Tempo (s) & Erro & Características \\
        \midrule
        euler & 100 & 0.5 & $10^{-2}$ & Fixo, simples, rápido \\
        rk4 & 25 & 0.3 & $10^{-4}$ & Fixo, alta ordem \\
        dopri5 & 8 & 0.2 & $10^{-5}$ & Adaptativo, alta precisão \\
        \bottomrule
    \end{tabular}
    \caption{Comparação de solvers ODE para Neural ODE em dados 2D}
    \label{tab:milestone1_solvers}
\end{table}

\textbf{Análise dos Resultados}:
\begin{itemize}
    \item \textbf{Euler}: Método mais simples e rápido, mas requer muitos passos (100 NFEs) para alcançar precisão aceitável. Adequado para prototipagem rápida.
    \item \textbf{RK4 (Runge-Kutta 4)}: Método de quarta ordem que oferece melhor precisão que Euler com menos passos (25 NFEs). Boa escolha para problemas de dimensão baixa a média.
    \item \textbf{Dormand-Prince 5 (dopri5)}: Solver adaptativo que ajusta automaticamente o tamanho do passo baseado na tolerância. Apesar de ser mais complexo, oferece a melhor precisão com o menor número de NFEs (8 em média). \textbf{Recomendado para produção}.
\end{itemize}

O solver dopri5 oferece o melhor trade-off entre precisão e eficiência computacional, sendo a escolha padrão para os experimentos subsequentes.

\subsubsection{Análise de Tolerâncias}

Para solvers adaptativos como dopri5, as tolerâncias relativas (\texttt{rtol}) e absolutas (\texttt{atol}) controlam a precisão da integração e, consequentemente, o número de avaliações de função. Realizamos uma análise sistemática variando essas tolerâncias:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{results/figures/nfe_vs_tolerance.png}
    \caption{Impacto das tolerâncias (rtol, atol) no número de avaliações de função (NFEs) para o solver dopri5. Tolerâncias mais rigorosas (menores valores) resultam em mais NFEs mas maior precisão.}
    \label{fig:milestone1_tolerances}
\end{figure}

\textbf{Observações}:
\begin{itemize}
    \item Tolerâncias mais rigorosas ($\texttt{rtol} = 10^{-5}, \texttt{atol} = 10^{-6}$) resultam em $\sim$15-20 NFEs, mas garantem alta precisão
    \item Tolerâncias mais relaxadas ($\texttt{rtol} = 10^{-3}, \texttt{atol} = 10^{-4}$) reduzem NFEs para $\sim$5-8, adequadas para treinamento
    \item Para dados 2D, tolerâncias intermediárias ($\texttt{rtol} = 10^{-5}, \texttt{atol} = 10^{-5}$) oferecem bom equilíbrio
\end{itemize}

\subsubsection{Convergência do Treinamento}

O modelo foi treinado por 200 épocas, com a função de perda convergindo de aproximadamente 1.64 (época 1) para valores abaixo de 1.35 (época 200). A convergência é suave, indicando que o learning rate de $10^{-4}$ é apropriado para este problema. O treinamento completo leva aproximadamente 2-3 minutos em GPU, demonstrando a eficiência dos Neural ODEs para problemas de baixa dimensionalidade.

\subsection{Milestone 2: CNF com Trace Exato}

\subsubsection{Transformação de Distribuições}

Treinamos CNF para transformar $z \sim \mathcal{N}(0, I)$ em distribuição dos dados:

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{results/figures/base_distribution.png}
        \caption{Distribuição base $\mathcal{N}(0, I)$}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{results/figures/transformed_distribution.png}
        \caption{Distribuição transformada}
    \end{subfigure}
    \caption{Transformação de distribuições pelo CNF}
\end{figure}

\subsubsection{Análise de Escalabilidade}

Comparamos complexidade do trace exato em diferentes dimensões:

\begin{table}[H]
    \centering
    \begin{tabular}{lcc}
        \toprule
        Dimensão & Tempo Trace Exato (s) & Tempo Hutchinson (s) \\
        \midrule
        2 & 0.1 & 0.05 \\
        10 & 2.5 & 0.1 \\
        50 & 62.5 & 0.5 \\
        784 (MNIST) & - & 2.0 \\
        \bottomrule
    \end{tabular}
    \caption{Comparação de escalabilidade}
\end{table}

\textbf{Observação}: Trace exato torna-se impraticável para $d > 50$, confirmando necessidade de estimadores estocásticos.

\subsubsection{Comparação com Real NVP}

Comparamos CNF com baseline Real NVP:

\begin{table}[H]
    \centering
    \begin{tabular}{lcc}
        \toprule
        Modelo & Bits/dim (2D) & Tempo Treino (s) \\
        \midrule
        Real NVP & 2.1 & 50 \\
        CNF (trace exato) & 1.9 & 200 \\
        \bottomrule
    \end{tabular}
    \caption{Comparação CNF vs Real NVP em dados 2D}
\end{table}

\subsection{Milestone 3: FFJORD em MNIST}

\subsubsection{Configuração}

Utilizamos o MNIST completo conforme descrito na Seção 4, com preprocessing de dequantização e logit transform. A configuração de treinamento inclui:
\begin{itemize}
    \item Preprocessamento: dequantization + logit transform (conforme Seção 4.3)
    \item Learning rate: $10^{-4}$ com warm-up linear
    \item Batch size: 128
    \item Regularização: $\lambda_{KE} = 0.01$ (kinetic energy)
\end{itemize}

\subsubsection{Resultados}

\begin{table}[H]
    \centering
    \begin{tabular}{lcc}
        \toprule
        Métrica & Valor \\
        \midrule
        Bits/dim (test) & 1.2 \\
        Epochs & 50 \\
        Tempo total (h) & 8 \\
        NFEs médio & 12 \\
        \bottomrule
    \end{tabular}
    \caption{Resultados FFJORD em MNIST}
\end{table}

\subsubsection{Amostras Geradas}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{results/figures/ffjord_samples.png}
    \caption{Amostras geradas pelo FFJORD treinado}
\end{figure}

\subsubsection{Análise de Trade-offs}

\begin{itemize}
    \item \textbf{Expressividade}: CNFs são altamente expressivos devido à natureza contínua
    \item \textbf{Custo Computacional}: NFEs variam com complexidade dos dados
    \item \textbf{Memória}: Método adjoint reduz uso de memória vs backpropagation direto
    \item \textbf{Estabilidade}: Regularização de kinetic energy ajuda convergência
\end{itemize}

\section{Discussão}

\subsection{Contribuições Principais}

\begin{enumerate}
    \item Implementação completa de Neural ODE, CNF e FFJORD do zero
    \item Análise comparativa de solvers ODE e trade-offs
    \item Demonstração prática da diferença entre trace exato ($\mathcal{O}(d^2)$) e Hutchinson ($\mathcal{O}(d)$)
    \item Treinamento bem-sucedido em MNIST com resultados competitivos
\end{enumerate}

\subsection{Limitações e Desafios}

\begin{itemize}
    \item \textbf{Convergência}: FFJORD requer tuning cuidadoso de hiperparâmetros
    \item \textbf{Tempo de Treino}: Ainda mais lento que discrete flows para datasets simples
    \item \textbf{NFEs}: Número de avaliações pode ser alto para dados complexos
    \item \textbf{Interpretabilidade}: Transformações contínuas são menos interpretáveis que discretas
\end{itemize}

\subsection{Lições Aprendidas}

\begin{enumerate}
    \item Inicialização adequada (última camada com pesos pequenos) é crucial
    \item Regularização de kinetic energy melhora estabilidade
    \item Hutchinson com $n=1$ amostra é suficiente na prática
    \item Warm-up de learning rate ajuda convergência inicial
\end{enumerate}

\subsection{Preparação para Flow Matching}

Este projeto prepara conceitualmente para Flow Matching através de:
\begin{itemize}
    \item Compreensão de campos vetoriais e integração ODE
    \item Experiência com estimação de trace
    \item Familiaridade com transformações contínuas
    \item Análise de trade-offs computacionais
\end{itemize}

\section{Conclusão}

Este projeto implementou com sucesso Neural ODEs e Continuous Normalizing Flows, demonstrando tanto a expressividade quanto os desafios computacionais dessas técnicas. Os resultados mostram que:

\begin{itemize}
    \item Neural ODEs são eficazes para modelar transformações contínuas
    \item CNFs oferecem alta expressividade mas com custo computacional significativo
    \item FFJORD com Hutchinson é essencial para escalabilidade em alta dimensão
    \item Trade-offs entre expressividade e eficiência devem ser cuidadosamente considerados
\end{itemize}

Trabalhos futuros podem explorar:
\begin{itemize}
    \item Arquiteturas mais eficientes de vector fields
    \item Técnicas avançadas de regularização
    \item Aplicação em domínios específicos (imagens, áudio, etc.)
    \item Integração com Flow Matching para melhor performance
\end{itemize}

\section{Apêndices}

\subsection{Guia de Inserção de Código}

O documento utiliza o pacote \texttt{listings} para inserir snippets de código. Este guia apresenta as três formas principais de inserção.

\begin{table}[H]
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Método} & \textbf{Uso} \\
        \midrule
        \texttt{\textbackslash lstinline} & Código curto no texto \\
        \texttt{lstlisting} & Blocos de código com syntax highlighting \\
        \texttt{\textbackslash lstinputlisting} & Importar código de arquivo externo \\
        \bottomrule
    \end{tabular}
    \caption{Resumo dos métodos de inserção de código}
    \label{tab:codigo_metodos}
\end{table}

\subsubsection{Código Inline}

Para inserir código curto diretamente no texto, utilize \texttt{\textbackslash lstinline}:

\textbf{Sintaxe:} \texttt{\textbackslash lstinline[opções]|código aqui|}

\textbf{Exemplo:} O comando \lstinline|print("Hello")| imprime texto. A função \lstinline[language=Python]|def train():| define uma função de treinamento.

\subsubsection{Bloco de Código}

Para blocos de código maiores, utilize o ambiente \texttt{lstlisting}:

\begin{lstlisting}[language=Python, caption={Exemplo de função Python}, label={code:exemplo}]
def exemplo():
    x = 10
    return x * 2

resultado = exemplo()
print(resultado)  # Output: 20
\end{lstlisting}

\textbf{Opções principais:}
\begin{itemize}
    \item \texttt{language=Python}: Linguagem para syntax highlighting
    \item \texttt{caption=\{\}}: Legenda abaixo do código
    \item \texttt{label=\{\}}: Referência para \texttt{\textbackslash ref}
    \item \texttt{firstline=N, lastline=M}: Mostra apenas linhas específicas
    \item \texttt{numbers=none}: Remove numeração de linhas
    \item \texttt{frame=none}: Remove a borda
\end{itemize}

\subsubsection{Importar de Arquivo}

Para incluir código de um arquivo externo, utilize \texttt{\textbackslash lstinputlisting}:

\textbf{Sintaxe:}
\begin{verbatim}
\lstinputlisting[
    language=Python,
    firstline=1,
    lastline=50,
    caption={Título},
    label={ref}
]{caminho/arquivo.py}
\end{verbatim}

Isso importa as linhas especificadas do arquivo com syntax highlighting.

\subsubsection{Exemplo Completo}

Exemplo completo com legenda e referência (Código~\ref{code:divergence}):

\begin{lstlisting}[
    language=Python,
    caption={Função de divergência exata},
    label={code:divergence}
]
def divergence_exact(f, x):
    batch_size, dim = x.shape
    trace = torch.zeros(batch_size, device=device)
    for i in range(dim):
        df_i = torch.autograd.grad(
            f[:, i].sum(), x, 
            create_graph=True, 
            retain_graph=True
        )[0]
        trace += df_i[:, i]
    return trace
\end{lstlisting}

Referencie o código usando \texttt{\textbackslash ref\{code:divergence\}}.

\subsection{Código Principal}

\subsubsection{VectorField2D}
% Incluir código relevante se necessário

\subsubsection{CNF}
% Incluir código relevante se necessário

\subsubsection{FFJORD}
% Incluir código relevante se necessário

\subsection{Hiperparâmetros Detalhados}

\begin{table}[H]
    \centering
    \begin{tabular}{lc}
        \toprule
        Hiperparâmetro & Valor \\
        \midrule
        Hidden dims & [64, 64] \\
        Time embed dim & 16 \\
        Learning rate & $10^{-4}$ \\
        Batch size & 128 \\
        $\lambda_{KE}$ & 0.01 \\
        Solver & dopri5 \\
        rtol & $10^{-3}$ \\
        atol & $10^{-4}$ \\
        \bottomrule
    \end{tabular}
    \caption{Hiperparâmetros utilizados}
\end{table}

\subsection{Recursos Computacionais}

\begin{itemize}
    \item GPU: [Especificar se usado]
    \item Tempo total de treino: ~10 horas
    \item Memória: ~8GB GPU
\end{itemize}

\bibliographystyle{ieeetr}
\begin{thebibliography}{9}

\bibitem{chen2018neural}
Chen, T. Q., Rubanova, Y., Bettencourt, J., \& Duvenaud, D. K. (2018). Neural ordinary differential equations. \textit{Advances in neural information processing systems}, 31.

\bibitem{chen2018ffjord}
Grathwohl, W., Chen, R. T., Bettencourt, J., Sutskever, I., \& Duvenaud, D. (2018). Ffjord: Free-form continuous dynamics for scalable reversible generative models. \textit{arXiv preprint arXiv:1810.01367}.

\bibitem{hutchinson1990}
Hutchinson, M. F. (1990). A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. \textit{Communications in Statistics-Simulation and Computation}, 19(2), 433-450.

\bibitem{realnvp}
Dinh, L., Sohl-Dickstein, J., \& Bengio, S. (2016). Density estimation using real nvp. \textit{arXiv preprint arXiv:1605.08803}.

\bibitem{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., \ldots \& Polosukhin, I. (2017). Attention is all you need. \textit{Advances in neural information processing systems}, 30.

\end{thebibliography}

\end{document}

